{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa74e16d",
      "metadata": {
        "id": "fa74e16d"
      },
      "source": [
        "# Classifying data into 5 categories Using SVM, Naive Bayes and Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim liwc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPVK8QUJV8nw",
        "outputId": "dd3eeb1b-fe0b-4d3b-af71-f8ef2c5fa8f4"
      },
      "id": "oPVK8QUJV8nw",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Collecting liwc\n",
            "  Downloading liwc-0.5.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading liwc-0.5.0-py2.py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: liwc\n",
            "Successfully installed liwc-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U liwc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MoHNLfZX5bX",
        "outputId": "ac9c7830-56d2-43d9-a888-197a98e31660"
      },
      "id": "6MoHNLfZX5bX",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: liwc in /usr/local/lib/python3.12/dist-packages (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c045ad2",
      "metadata": {
        "id": "4c045ad2"
      },
      "source": [
        "# Training the Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from gensim.models import FastText\n",
        "import liwc\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pickle # Import the pickle library\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"2024_Drug_Data.csv\")\n",
        "\n",
        "\n",
        "# Feature Extraction\n",
        "X = data['body']\n",
        "y = data['label_classification']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_and_tokenize(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # Remove tweet handles\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove symbols\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove hyperlinks\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return re.findall(r'\\w+', text)  # Tokenize\n",
        "\n",
        "# Initialize Sentiment Intensity Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Extract sentiment polarity features for each post\n",
        "X_sentiment_train = []\n",
        "X_sentiment_test = []\n",
        "for post in X_train:\n",
        "    sentiment_scores = sid.polarity_scores(post)\n",
        "    X_sentiment_train.append([sentiment_scores['neg'], sentiment_scores['pos']])\n",
        "for post in X_test:\n",
        "    sentiment_scores = sid.polarity_scores(post)\n",
        "    X_sentiment_test.append([sentiment_scores['neg'], sentiment_scores['pos']])\n",
        "\n",
        "# Convert sentiment features to DataFrame\n",
        "X_sentiment_train = pd.DataFrame(X_sentiment_train, columns=['neg_score', 'pos_score'])\n",
        "X_sentiment_test = pd.DataFrame(X_sentiment_test, columns=['neg_score', 'pos_score'])\n",
        "\n",
        "# Initialize LIWC analyzer\n",
        "# Assuming LIWC functionality is encapsulated in a function called load_token_parser\n",
        "liwc_analyzer, category_names = liwc.load_token_parser('LIWC2007_English100131.dic')\n",
        "\n",
        "# Extract LIWC-based features for training data\n",
        "X_liwc_train = []\n",
        "X_liwc_test = []\n",
        "\n",
        "##1.8. Lemmatizing Each token\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "\n",
        "for post in X_train:\n",
        "    words = clean_and_tokenize(post)\n",
        "    # Lemmatize the post into words\n",
        "    words = lemmatize(words)\n",
        "    liwc_counts = Counter()\n",
        "    for word in words:\n",
        "        for category in liwc_analyzer(word):\n",
        "            if category in category_names:\n",
        "                liwc_counts[category] += 1\n",
        "    X_liwc_train.append(list(liwc_counts.values()))\n",
        "for post in X_test:\n",
        "    words = clean_and_tokenize(post)\n",
        "    # Lemmatize the post into words\n",
        "    words = lemmatize(words)\n",
        "    liwc_counts = Counter()\n",
        "    for word in words:\n",
        "        for category in liwc_analyzer(word):\n",
        "            if category in category_names:\n",
        "                liwc_counts[category] += 1\n",
        "    X_liwc_test.append(list(liwc_counts.values()))\n",
        "\n",
        "# Convert LIWC features to DataFrame\n",
        "X_liwc_train = pd.DataFrame(X_liwc_train, columns=category_names)\n",
        "X_liwc_test = pd.DataFrame(X_liwc_test, columns=category_names)\n",
        "\n",
        "# Unigram and bigram features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
        "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Word-based Features\n",
        "word_glossaries = ['health', 'low self-esteem stress', 'recovery', 'addiction']\n",
        "vectorizer = CountVectorizer(vocabulary=word_glossaries)\n",
        "X_word_train = vectorizer.fit_transform(X_train)\n",
        "X_word_test = vectorizer.transform(X_test)\n",
        "X_word_dense_train = X_word_train.toarray()\n",
        "X_word_dense_test = X_word_test.toarray()\n",
        "\n",
        "# Concatenate all features\n",
        "X_features_train = np.concatenate((X_liwc_train, X_sentiment_train, X_tfidf_train.toarray(), X_word_dense_train), axis=1)\n",
        "X_features_test = np.concatenate((X_liwc_test, X_sentiment_test, X_tfidf_test.toarray(), X_word_dense_test), axis=1)\n",
        "\n",
        "# Impute missing values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed_train = imputer.fit_transform(X_features_train)\n",
        "X_imputed_test = imputer.transform(X_features_test)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled_train = scaler.fit_transform(X_imputed_train)\n",
        "X_scaled_test = scaler.transform(X_imputed_test)\n",
        "\n",
        "# Data Balancing using SMOTE\n",
        "smote = SMOTE()\n",
        "X_resampled_train, y_resampled_train = smote.fit_resample(X_scaled_train, y_train)\n",
        "\n",
        "# Add this import at the top\n",
        "import json\n",
        "\n",
        "# --- SAVE PREPROCESSING PARAMETERS AS JSON (instead of pickle) ---\n",
        "preprocessing_params = {\n",
        "    'tfidf_max_features': int(2000),  # Ensure it's Python int\n",
        "    'tfidf_ngram_range': [1, 2],\n",
        "    'tfidf_vocabulary': {str(k): int(v) for k, v in tfidf_vectorizer.vocabulary_.items()},  # Convert keys to str, values to int\n",
        "    'tfidf_idf_': [float(x) for x in tfidf_vectorizer.idf_],  # Convert to Python floats\n",
        "\n",
        "    'word_glossaries': word_glossaries,\n",
        "    'word_vocabulary': {str(k): int(v) for k, v in vectorizer.vocabulary_.items()},\n",
        "\n",
        "    'imputer_strategy': 'mean',\n",
        "    'imputer_statistics_': [float(x) for x in imputer.statistics_],\n",
        "    'imputer_n_features_in_': int(imputer.n_features_in_),  # Add this\n",
        "\n",
        "    'scaler_min_': [float(x) for x in scaler.min_],\n",
        "    'scaler_scale_': [float(x) for x in scaler.scale_],\n",
        "    'scaler_data_min_': [float(x) for x in scaler.data_min_],\n",
        "    'scaler_data_max_': [float(x) for x in scaler.data_max_],\n",
        "    'scaler_data_range_': [float(x) for x in scaler.data_range_],\n",
        "    'scaler_n_features_in_': int(scaler.n_features_in_),  # Add this\n",
        "\n",
        "    'category_names': list(category_names),  # Ensure it's a Python list\n",
        "    'feature_names_order': ['liwc', 'sentiment', 'tfidf', 'word']\n",
        "}\n",
        "\n",
        "# Save as JSON (much smaller than pickle)\n",
        "with open('preprocessing_params.json', 'w') as file:\n",
        "    json.dump(preprocessing_params, file, indent=2)\n",
        "\n",
        "print(\"Preprocessing parameters saved as 'preprocessing_params.json'\")\n",
        "\n",
        "# --- CLASSIFIER MODELS ---\n",
        "classifiers = {\n",
        "    \"SVM\": SVC(kernel='linear'),\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=3000),\n",
        "}\n",
        "\n",
        "for name, classifier in classifiers.items():\n",
        "    classifier.fit(X_resampled_train, y_resampled_train)\n",
        "    y_pred = classifier.predict(X_scaled_test)\n",
        "    print(f\"Classification Report for {name}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"Confusion Matrix for {name}:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"----------------------------------------------\")\n",
        "\n",
        "    # Save the trained model\n",
        "    model_filename = f'trained_{name.lower().replace(\" \", \"_\")}_model.pkl'\n",
        "    with open(model_filename, 'wb') as file:\n",
        "        pickle.dump(classifier, file)\n",
        "    print(f\"Model saved as '{model_filename}'\")\n",
        "\n",
        "    # Save model classes for the Logistic Regression model\n",
        "    if name == \"Logistic Regression\":\n",
        "        model_classes = {\n",
        "            'classes': classifier.classes_.tolist()\n",
        "        }\n",
        "        with open('model_classes.json', 'w') as file:\n",
        "            json.dump(model_classes, file, indent=2)\n",
        "        print(f\"Model classes saved: {classifier.classes_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7If0U_TnopmU",
        "outputId": "6c672dde-5a37-4673-b759-08ec6976df0e"
      },
      "id": "7If0U_TnopmU",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing parameters saved as 'preprocessing_params.json'\n",
            "Classification Report for SVM:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  A-Recovery       0.27      0.32      0.30        37\n",
            "    Addicted       0.81      0.83      0.82       266\n",
            "  E-Recovery       0.55      0.54      0.54       138\n",
            "  M-Recovery       0.56      0.53      0.54       127\n",
            "      Others       0.69      0.65      0.67        63\n",
            "\n",
            "    accuracy                           0.66       631\n",
            "   macro avg       0.58      0.57      0.58       631\n",
            "weighted avg       0.66      0.66      0.66       631\n",
            "\n",
            "Confusion Matrix for SVM:\n",
            "[[ 12   5   9  11   0]\n",
            " [ 12 222  13   7  12]\n",
            " [  8  22  74  30   4]\n",
            " [  9  15  34  67   2]\n",
            " [  3  10   5   4  41]]\n",
            "----------------------------------------------\n",
            "Model saved as 'trained_svm_model.pkl'\n",
            "Classification Report for Gaussian Naive Bayes:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  A-Recovery       0.24      0.27      0.26        37\n",
            "    Addicted       0.81      0.83      0.82       266\n",
            "  E-Recovery       0.57      0.54      0.56       138\n",
            "  M-Recovery       0.46      0.55      0.50       127\n",
            "      Others       0.79      0.43      0.56        63\n",
            "\n",
            "    accuracy                           0.64       631\n",
            "   macro avg       0.58      0.52      0.54       631\n",
            "weighted avg       0.65      0.64      0.64       631\n",
            "\n",
            "Confusion Matrix for Gaussian Naive Bayes:\n",
            "[[ 10   3   6  18   0]\n",
            " [  4 221  20  15   6]\n",
            " [  8  14  75  40   1]\n",
            " [ 14  18  25  70   0]\n",
            " [  5  17   5   9  27]]\n",
            "----------------------------------------------\n",
            "Model saved as 'trained_gaussian_naive_bayes_model.pkl'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  A-Recovery       0.34      0.32      0.33        37\n",
            "    Addicted       0.85      0.84      0.84       266\n",
            "  E-Recovery       0.58      0.57      0.58       138\n",
            "  M-Recovery       0.60      0.58      0.59       127\n",
            "      Others       0.67      0.76      0.71        63\n",
            "\n",
            "    accuracy                           0.69       631\n",
            "   macro avg       0.61      0.62      0.61       631\n",
            "weighted avg       0.69      0.69      0.69       631\n",
            "\n",
            "Confusion Matrix for Logistic Regression:\n",
            "[[ 12   5  11   9   0]\n",
            " [  5 224  12   8  17]\n",
            " [  7  19  79  29   4]\n",
            " [ 11  10  29  74   3]\n",
            " [  0   7   5   3  48]]\n",
            "----------------------------------------------\n",
            "Model saved as 'trained_logistic_regression_model.pkl'\n",
            "Model classes saved: ['A-Recovery' 'Addicted' 'E-Recovery' 'M-Recovery' 'Others']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding Unique Drugs and Related Mentor Data\n"
      ],
      "metadata": {
        "id": "3OwZ1uAPbaLW"
      },
      "id": "3OwZ1uAPbaLW"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ab33f08d-0d1d-4495-b48a-41655b6b2c67",
      "metadata": {
        "id": "ab33f08d-0d1d-4495-b48a-41655b6b2c67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "a3b10d83-a712-48f6-b85d-66f47bdd8d5d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'drug_names.tsv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1009252866.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Read drug names from the TSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drug_names.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdrug_names_sider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"carnitine\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drug_names.tsv'"
          ]
        }
      ],
      "source": [
        "# Finding Drugs from Each Post\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read drug names from the TSV file\n",
        "file_path = 'drug_names.tsv'\n",
        "data = pd.read_csv(file_path, sep='\\t')\n",
        "drug_names_sider = [\"carnitine\"] + data.iloc[:, 1].tolist()\n",
        "\n",
        "# Read drug names from the CSV file\n",
        "file_path_csv = 'drugsComTest_raw.csv'\n",
        "df = pd.read_csv(file_path_csv)\n",
        "\n",
        "# Extract drug names column\n",
        "drug_names_csv = df['drugName'].str.lower().tolist()\n",
        "\n",
        "# Combine the drug names from both sources\n",
        "combined_drug_names = drug_names_sider + drug_names_csv\n",
        "\n",
        "# Remove duplicates and convert to lowercase\n",
        "unique_drug_names = list(set(map(str.lower, combined_drug_names)))\n",
        "\n",
        "# Write the unique drug names to a new CSV file\n",
        "unique_drugs_df = pd.DataFrame(unique_drug_names, columns=['DrugName'])\n",
        "unique_drugs_df.to_csv('unique_drug_names.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a476bf9e",
      "metadata": {
        "id": "a476bf9e"
      },
      "source": [
        "Checking the Drug Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2d74bd6b",
      "metadata": {
        "id": "2d74bd6b",
        "outputId": "497274af-0989-456a-a869-8034fa6d29fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             DrugName\n",
              "0        remifentanil\n",
              "1  ortho-novum 1 / 35\n",
              "2         cosyntropin\n",
              "3              femara\n",
              "4           poly iron"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8031ff10-a52e-48b2-a906-80a3d73e342b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DrugName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>remifentanil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ortho-novum 1 / 35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cosyntropin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>femara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>poly iron</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8031ff10-a52e-48b2-a906-80a3d73e342b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8031ff10-a52e-48b2-a906-80a3d73e342b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8031ff10-a52e-48b2-a906-80a3d73e342b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5fd79816-1167-47ce-93af-d0ea2dae4af5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5fd79816-1167-47ce-93af-d0ea2dae4af5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5fd79816-1167-47ce-93af-d0ea2dae4af5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "drug",
              "summary": "{\n  \"name\": \"drug\",\n  \"rows\": 3418,\n  \"fields\": [\n    {\n      \"column\": \"DrugName\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3418,\n        \"samples\": [\n          \"phentermine / topiramate\",\n          \"cyanocobalamin\",\n          \"acipimox\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Load the data from the uploaded file to see its structure and contents.\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "drug = pd.read_csv(\"unique_drug_names.csv\")\n",
        "drug.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5f20581a",
      "metadata": {
        "id": "5f20581a",
        "outputId": "bf86afce-0f60-4a68-cfcb-5f423e6ac4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       12\n",
              "1       18\n",
              "2       11\n",
              "3        6\n",
              "4        9\n",
              "        ..\n",
              "3413    10\n",
              "3414     5\n",
              "3415    13\n",
              "3416    10\n",
              "3417    17\n",
              "Name: DrugName, Length: 3418, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DrugName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3413</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3414</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3415</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3416</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3417</th>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3418 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "drug[\"DrugName\"].str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a057bed2",
      "metadata": {
        "id": "a057bed2",
        "outputId": "2f0d6c34-75f9-4b84-a52f-1cd20f1f6783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3970614744.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Add a new column to store mentioned drugs for each post\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mreddit_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mentioned Drugs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreddit_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfind_mentioned_drugs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DrugName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Create a new CSV file with post information and mentioned drugs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3970614744.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Add a new column to store mentioned drugs for each post\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mreddit_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mentioned Drugs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreddit_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfind_mentioned_drugs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DrugName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Create a new CSV file with post information and mentioned drugs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3970614744.py\u001b[0m in \u001b[0;36mfind_mentioned_drugs\u001b[0;34m(post_text, drug_list)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdrug\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdrug_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Using regular expression to find drug names in post text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\b{}\\b'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mmentioned_drugs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmentioned_drugs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/re/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    176\u001b[0m     a Match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/re/__init__.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    306\u001b[0m                     DeprecationWarning)\n\u001b[1;32m    307\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/enum.py\u001b[0m in \u001b[0;36m__and__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{flag}' cannot be combined with other flags with &\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/enum.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m   1541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def find_mentioned_drugs(post_text, drug_list):\n",
        "    mentioned_drugs = []\n",
        "    post_text = post_text.lower()\n",
        "    for drug in drug_list:\n",
        "        # Using regular expression to find drug names in post text\n",
        "        if re.search(r'\\b{}\\b'.format(re.escape(drug.lower())), post_text):\n",
        "            mentioned_drugs.append(drug)\n",
        "    return mentioned_drugs\n",
        "\n",
        "# Load Reddit posts CSV\n",
        "reddit_df = pd.read_csv(\"2024_Drug_Data.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# Add a new column to store mentioned drugs for each post\n",
        "reddit_df['Mentioned Drugs'] = reddit_df['body'].apply(lambda x: find_mentioned_drugs(x, drug['DrugName']))\n",
        "\n",
        "# Create a new CSV file with post information and mentioned drugs\n",
        "new_csv_file = 'reddit_posts_with_drugs.csv'\n",
        "reddit_df.to_csv(new_csv_file, index=False)\n",
        "\n",
        "print(\"New CSV file created with mentioned drugs:\", new_csv_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Mentor and Drug Data\n"
      ],
      "metadata": {
        "id": "Botp7QwCbwgK"
      },
      "id": "Botp7QwCbwgK"
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess_mentor_data.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import liwc\n",
        "import nltk\n",
        "import pickle\n",
        "import ast\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data files\n",
        "print(\"Loading data files...\")\n",
        "data = pd.read_csv(\"2024_Drug_Data.csv\")\n",
        "drug_names_df = pd.read_csv(\"unique_drug_names.csv\")\n",
        "drug_list = drug_names_df['DrugName'].tolist()\n",
        "\n",
        "# Initialize LIWC analyzer (static tool)\n",
        "try:\n",
        "    liwc_analyzer, category_names = liwc.load_token_parser('LIWC2007_English100131.dic')\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"LIWC dictionary file missing.\")\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "def find_mentioned_drugs(post_text, drugs):\n",
        "    mentioned = []\n",
        "    if not isinstance(post_text, str):\n",
        "        return mentioned\n",
        "    post_text = post_text.lower()\n",
        "    for drug in drugs:\n",
        "        if re.search(r'\\b{}\\b'.format(re.escape(drug.lower())), post_text):\n",
        "            mentioned.append(drug)\n",
        "    return mentioned\n",
        "\n",
        "# --- Perform the slow processing here ---\n",
        "print(\"Processing data to find mentioned drugs...\")\n",
        "data['Mentioned Drugs'] = data['body'].apply(lambda x: find_mentioned_drugs(x, drug_list))\n",
        "print(\"Finding mentioned drugs complete.\")\n",
        "\n",
        "# Identify potential mentors from users in recovery stages\n",
        "recovery_users = data[data['label_classification'].isin(['E-Recovery', 'M-Recovery', 'A-Recovery'])]\n",
        "\n",
        "# Create a dictionary to hold the recovery information for users\n",
        "recovery_dict = defaultdict(list)\n",
        "for _, row in recovery_users.iterrows():\n",
        "    drugs_recovered_from = row['Mentioned Drugs']\n",
        "    if drugs_recovered_from:\n",
        "        recovery_dict[row['username']].extend(drugs_recovered_from)\n",
        "\n",
        "# Save the preprocessed data to a file\n",
        "with open('mentor_data.pkl', 'wb') as f:\n",
        "    pickle.dump(recovery_dict, f)\n",
        "\n",
        "print(\"Mentor data successfully preprocessed and saved to 'mentor_data.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4uprBl0A-45",
        "outputId": "cf78e672-c513-400c-951f-f13fe01ab6ff"
      },
      "id": "l4uprBl0A-45",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data files...\n",
            "Processing data to find mentioned drugs...\n",
            "Finding mentioned drugs complete.\n",
            "Mentor data successfully preprocessed and saved to 'mentor_data.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a65d964",
      "metadata": {
        "id": "2a65d964"
      },
      "source": [
        "# Drug mentor Determination"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b21070e",
      "metadata": {
        "id": "5b21070e"
      },
      "source": [
        "Recommending the users who wish to revive from their addiction names of users who have recovered from the same drugs and are under Maintaining recovery stage(M-recovery) or Advanced recovery stage(A-recovery)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a98db9-b69c-4fd0-8868-73b6b070f652",
      "metadata": {
        "id": "13a98db9-b69c-4fd0-8868-73b6b070f652",
        "outputId": "c9b93348-73e0-4e48-d128-d48ca27f2503"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Kulutkarsh\\AppData\\Local\\Temp\\ipykernel_8064\\2958216981.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  intention_to_recover['Mentioned Drugs'] = intention_to_recover['Mentioned Drugs'].apply(ast.literal_eval)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>Mentioned Drugs</th>\n",
              "      <th>Top 3 Mentors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NAME_2</td>\n",
              "      <td>[cocaine, amphetamine]</td>\n",
              "      <td>[NAME_768, NAME_831, NAME_918]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NAME_3</td>\n",
              "      <td>[percocet, valium]</td>\n",
              "      <td>[NAME_768, NAME_916, NAME_1084]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NAME_7</td>\n",
              "      <td>[adderall]</td>\n",
              "      <td>[NAME_976, NAME_1282, NAME_1312]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NAME_10</td>\n",
              "      <td>[ketamine]</td>\n",
              "      <td>[NAME_1629]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NAME_11</td>\n",
              "      <td>[nitrous]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   username         Mentioned Drugs                     Top 3 Mentors\n",
              "2    NAME_2  [cocaine, amphetamine]    [NAME_768, NAME_831, NAME_918]\n",
              "3    NAME_3      [percocet, valium]   [NAME_768, NAME_916, NAME_1084]\n",
              "7    NAME_7              [adderall]  [NAME_976, NAME_1282, NAME_1312]\n",
              "10  NAME_10              [ketamine]                       [NAME_1629]\n",
              "11  NAME_11               [nitrous]                                []"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import ast\n",
        "\n",
        "# Step 1: Identify users who intend to recover\n",
        "# Filter out rows where users show intention to recover\n",
        "intention_to_recover = data[data['label_recommendation'] == 'Addicted with intention to recover']\n",
        "\n",
        "# Step 2: Extract the list of drugs for those who wish to recover\n",
        "# Convert the 'Mentioned Drugs' from string to actual list\n",
        "intention_to_recover['Mentioned Drugs'] = intention_to_recover['Mentioned Drugs'].apply(ast.literal_eval)\n",
        "intention_to_recover = intention_to_recover[intention_to_recover['Mentioned Drugs'].map(bool)]\n",
        "# Step 3: Identify potential mentors from users who have recovered\n",
        "# Filter out users who are in recovery classes\n",
        "recovery_users = data[data['label_recommendation'].isin(['E-recovery', 'M-Recovery', 'A-Recovery'])]\n",
        "\n",
        "# Create a dictionary to hold the recovery information for users\n",
        "recovery_dict = defaultdict(list)\n",
        "for _, row in recovery_users.iterrows():\n",
        "    drugs_recovered_from = ast.literal_eval(row['Mentioned Drugs'])\n",
        "    if drugs_recovered_from:  # We only consider users who have mentioned specific drugs\n",
        "        recovery_dict[row['username']].extend(drugs_recovered_from)\n",
        "\n",
        "# Step 4: Create a function to match and rank mentors for a user wishing to recover\n",
        "def find_mentors_for_user(user_row):\n",
        "    user_drugs = set(user_row['Mentioned Drugs'])\n",
        "    if not user_drugs:\n",
        "        return []\n",
        "\n",
        "    # Calculate the number of drugs in common with potential mentors\n",
        "    common_drugs_count = {\n",
        "        mentor: len(user_drugs.intersection(set(mentor_drugs)))\n",
        "        for mentor, mentor_drugs in recovery_dict.items()\n",
        "    }\n",
        "\n",
        "    # Filter out mentors with zero drugs in common\n",
        "    potential_mentors = {mentor: count for mentor, count in common_drugs_count.items() if count > 0}\n",
        "\n",
        "    # Rank mentors based on the number of drugs in common\n",
        "    ranked_mentors = sorted(potential_mentors.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Return the sorted list of potential mentors\n",
        "    return [mentor for mentor, _ in ranked_mentors]\n",
        "\n",
        "# Apply the function to each user wishing to recover to find their potential mentors\n",
        "intention_to_recover['Potential Mentors'] = intention_to_recover.apply(find_mentors_for_user, axis=1)\n",
        "\n",
        "# Display the first few entries to verify\n",
        "intention_to_recover['Top 3 Mentors'] = intention_to_recover['Potential Mentors'].apply(\n",
        "    lambda x: x[:3] if len(x) >= 3 else x\n",
        ")\n",
        "\n",
        "# Display the dataframe with the top 3 potential mentors\n",
        "intention_to_recover[['username', 'Mentioned Drugs', 'Top 3 Mentors']].head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}